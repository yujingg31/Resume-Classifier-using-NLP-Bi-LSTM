{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90758baf",
   "metadata": {},
   "source": [
    "### 1. Extracting text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF using PyPDF2.\"\"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \" \".join([page.extract_text().replace(\"\\n\", \" \") for page in reader.pages if page.extract_text()]) # Replace \\n with space\n",
    "    return text\n",
    "\n",
    "def load_dataset(base_dir):\n",
    "    \"\"\"Load PDF files and extract text from each category.\"\"\"\n",
    "    data = {}\n",
    "\n",
    "    for category in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, category)\n",
    "        if os.path.isdir(folder_path):\n",
    "            data[category] = []\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(folder_path, file)\n",
    "                    text = extract_text_from_pdf(pdf_path)\n",
    "                    data[category].append(text)\n",
    "            print(f\"Loaded {len(data[category])} files from category '{category}'\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load train dataset\n",
    "dataset = load_dataset(\"dataset/data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83c0407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a sample extracted text from ACCOUNTANT category\n",
    "print(dataset[\"ACCOUNTANT\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07778641",
   "metadata": {},
   "source": [
    "### 2. Convert data into Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77939cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Category                                        Resume_Text\n",
      "0     ACCOUNTANT  ACCOUNTANT Summary Financial Accountant specia...\n",
      "1     ACCOUNTANT  STAFF ACCOUNTANT Summary Highly analytical and...\n",
      "2     ACCOUNTANT  ACCOUNTANT Professional Summary To obtain a po...\n",
      "3     ACCOUNTANT  SENIOR ACCOUNTANT Experience Company Name   Ju...\n",
      "4     ACCOUNTANT  SENIOR ACCOUNTANT Professional Summary Senior ...\n",
      "...          ...                                                ...\n",
      "2479     TEACHER  READING TEACHER Summary I am a highly motivate...\n",
      "2480     TEACHER  HISTORY TEACHER Professional Summary To be emp...\n",
      "2481     TEACHER  TEACHER Summary Highly ethical, dependable, an...\n",
      "2482     TEACHER  TEACHER Summary Talented early education profe...\n",
      "2483     TEACHER  Kpandipou Koffi Summary Compassionate teaching...\n",
      "\n",
      "[2484 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert into dataframe\n",
    "import pandas as pd\n",
    "def convert_to_dataframe(dataset):\n",
    "    \"\"\"Convert the dataset dictionary to a pandas DataFrame.\"\"\"\n",
    "    records = []\n",
    "    for category, texts in dataset.items():\n",
    "        for text in texts:\n",
    "            records.append({\"Category\": category, \"Resume_Text\": text})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Convert dataset to DataFrame\n",
    "data_df = convert_to_dataframe(dataset)\n",
    "\n",
    "# Preview\n",
    "print(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451e392",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91c20587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shany\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shany\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shany\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accountant summary financial accountant specializing financial planning reporting analysis within department defense highlight account reconciliation result oriented financial reporting critical thinking accounting operation professional analysis financial system erp enterprise resource planning software excellent facilitator accomplishment served tiger team identified resolved general ledger posting deams totaling b accounting adjustment allowed first successful fiscal year end close collaboration dfas europe developed automated tool identified duplicate obligation tool allowed hq usafe deobligate duplicate obligation experience company name july november accountant city state enterprise resource planning office ero position accountant assigned defense enterprise accounting management system deams ero responsible identifying resolving issue affecting deams general ledger worked teammate procure pay order cash budget report area resolve daily challenge encountered deployment deams additional customer system change request promoted production supported testing script patch system change request ensuring anomaly identified deams functional management office action deams program management office system integrator addition served tiger team designed identify resolve general ledger posting difference supported development b accounting adjustment allowing first successful fiscal year end close action also allowed reconciliation closure fiscal year accounting adjustment ensuring deams fiscal year end requirement completed action recognized critical successful review report issued air force operational test evaluation center afotec resulting air force receiving authority continue deployment deams company name april june resource advisor city state position resource advisor st air communication operation squadron acos responsible providing financial advice decision support commander responsible coordinating budget four funding source coordinated usafe directorate intelligence usafe usafe directorate air space operation usafe usafe directorate communication usafe th air ground operation wing ensure acos meet mission requirement consistently managed three separate timeline providing budget unfunded requirement providing documentation various report required format organization discussed outcome group directorate budget meeting providing feedback day flight chief cc issue affect acos directly monitored defense travel system dts daily identify order authorization needing approval provided notification appropriate reviewing official approver utilizing dts general accounting finance system reviewed status report identify anomaly obligation identified order require correction prior payment provided government purchase card gpc status report day requested addition communicated appropriate cardholder change required support program identified cardholder training requirement monitored requirement ensure required training completed support mission critical program developed guidance gpc cardholder procedure requesting training squadron addition provided answer cardholder question unique non standard issue concern assumed role billing official final rating period completed self inspection program management control program zero finding yearly audit th con received zero finding company name july april staff accountant city state position staff accountant hq usafe responsible providing accounting financial oversight advice customer throughout command support usafe comptroller responsible performing ongoing analysis financial program identify negative trend weakness ensured specific weakness corrected determined whether systemic repeat issue identified adequately addressed required apply comprehensive knowledge analysis reporting requirement data produced resolve issue collaboration dfas europe developed automated tool identifies duplicate obligation comparing record accounting system contracting system provided notification fund manager review resolution tool eliminated hour manual research result allowed hq usafe deobligate duplicate obligation responsible establishing various performance metric ensured effective efficient use usafe financial resource supported usafe fma financial metric program collaborating dfas limestone development automated tool provided senior leader visibility usafe unit compliance established rule regulation related gpc tool provides management report used populate monthly metric chart briefed usafe fma tool provided capability usafe fma collaborate usafe contracting develop deploy joint guidance support established air force instruction mandating card suspension card holder compliance required reservation fund entitlement system support gpc identified resolved problem five gpc account rejecting automated interface process month research revealed account rejecting invalid paying station required manual intervention wing dfas personnel created rework delayed payment invoice partnered dfas denver corrected record access line account eliminating error condition identified method deliver one one training support usafe deployment open document analysis oda tool fmsuite utilizing defense connect online provided training remotely virtually eliminating need expend fund temporary duty tdy travel result training produced result went well expectation noted oda program management office company name january july chief report branch account maintenance control city state position chief report branch account maintenance control c responsible ensuring development standardization various managerial system report responsible completeness accuracy weekly monthly quarterly semi annual annual report branch monitored error general accounting finance system gafs bq ensured corrective action accomplished also ensured fund balance reconciled report verified prior release base activity higher headquarters limestone reorganized high performing organization hpo january time reassigned c directorate previously exist challenge time staff branch implement aggressive training schedule ensure continuity financial reporting maintained transitioned hpo continued defining mission function c entire network participated biweekly conference call standard compliance effort define mission function c worked management determining ftes needed branch responsible developing meaningful performance standard employee since branch function previously exist limestone poc initiative eliminate suspense account throughout agency identified suspense account initially targeted formulated strategy eliminate account requested waiver participated plan modify process using suspense account interfund suspense account action provided initial progress towards meeting department treasury mandate discontinue suspense account february worked staff reduce reconciliation million february million august accomplished despite loss experienced personnel realigning resource support critical initiative account payable orchestrated transition reporting requirement transportation financial management system tfms workload dfas omaha limestone transition limestone encouraged staff responsible report streamline process staff automated completely manual time consuming process thus eliminating potential key stroke error manually validating numerous spreadsheet listing contributor federal manager financial integrity act fmfia compliance review establishment assessable unit identified inconsistency information provided staff foreign currency fluctuation adjustment persisted getting higher level review regulatory policy guidance report foreign currency fluctuation consistently accurate company name february january chief account payable branch city state chief account payable responsible overall management branch consisting employee four first line supervisor responsible establishing priority schedule work assignment ensuring change workload accounted minimize impact normal office operation consistently reviewed area made necessary personnel move based shifting priority extremely important dfas denver directed database consolidation assumption air national guard workload workload increased rapidly staffing increased gradually dictated frequent priority change personnel move also worked closely major command supported dfas limestone strengthening partnership workload spike negatively impacted customer december overaged invoice percentage nearing backlog vendor pay document exceeding working dfas command client executive major command comptroller instrumental forming strategy included soliciting air force personnel assistance document processing identification must pay bill formation special action response team dedicated responding customer urgent requirement result effort three month period able reduce overaged invoice percentage backlog document document day old thereby minimizing adverse impact customer fund responsible providing personnel feedback session quarterly prepared supervisory appraisal employee performance potential advancement partnering management staff employee instrumental establishing employee performance plan linked employee performance established dfas strategy balance scorecard goal position supervisory accountant responsible performing ongoing analysis vendor pay workflow production identify negative trend weakness ensure specific weakness corrected determine whether systemic repeat issue identified adequately addressed required apply comprehensive knowledge analysis reporting requirement work process vendor pay system structure data produced resolve issue utilizing expertise louis ii data retrieval software produced ad hoc data query house external use customer retrieval designed reduce man hour necessary perform complex finance accounting function dfas air force personnel responsible budget resource necessary operate branch capacity prepared budget execution justification plan monitor overtime cost control supply purchase ensure cost efficient operation possible required respond inquiry various source include limited vendor dfas management accounting liaison office resource advisor dfas field site inquiry required ability relay technical aspect system deficiency customer familiar operation participated video teleconference conference call briefing designed address customer dfas management requirement called upon explain layman term dfas policy procedure regard delay payment due various reason responded various audit report study ensuring senior management audit personnel understand particular situation within vendor pay business process result finding company name february february chief recon report branch city state position chief vendor pay report recon branch exercised supervision either directly indirectly employee primarily series grade ranging g g responsibility also included supervision german local national worker assigned duty section responsible planning directing supervising activity work force review interpretation processing reconciliation vendor pay accounting data production timely accurate financial statement report requirement participated development branch policy continually reviewing evaluating organizational operation work distribution procedure coordinated activity assigned function organization obtain effective correlation financial data directed provided technical guidance subordinate assigned area assured timeliness accuracy assigned workload planned organized directed coordinated reviewed work subordinate section ensuring mission function division carried managed realigned resource conducted program analysis made decision accordance unit cost principle output target changing budgetary constraint participated long range planning goal setting evaluating subordinate staff interpreted clarified branch policy resolved operational problem ensured efficient utilization professional development staff expected provide reasonable assurance operation conducted compliance applicable law fund property asset safeguarded waste loss unauthorized use misappropriation ensured continuing affirmative application support dod dfas policy concerning equal opportunity affirmative action program ensured personnel management within organizational entity supervision accomplished without regard race color religion sex age national origin handicap kept abreast development policy issuance similar material equal opportunity field fully supported dod dfas equal opportunity program responsible accountable safety health subordinate ensured personnel trained work safely enforced safety health rule corrected unsafe unhealthy act unsafe unhealthy mechanical physical condition investigated mishap tool action necessary ensure safety health employee company name june february chief account payable branch city state responsible establishing priority schedule work assignment ensuring change workload accounted minimize impact normal office operation important dfas denver directed workload realighment field site servicing customer major command workload increased dictated frequent priority change personnel move also worked closely major command supported dfas limestone strengthening partnership workload spike negatively impacted customer responsible providing personnel feedback session quarterly preparedsupervisory appraisal employee performance potential advancement supervisory accountant responsible performing ongoing analysis vendor pay workflow production identified negative trend weakness ensured specific weakness corrected determine whether systemic repeat issue identified adequately addressed required apply comprehensive knowledge analysis reporting requirement work process vendor pay system structure data produced resolve issue utilizing knowledge louis ii data retrieval software produced ad hoc data query house external use customer retrieval designed reduce man hour necessary perform complex finance accounting function dfas air force personnel responsible budget resource necessary operate branch capacity prepared budget execution justification plan monitor overtime cost control supply purchase ensure cost efficient operation possible required respond inquiry various source include limited vendor dfas management accounting liaison office resource advisor dfas field site inquiry require ability relay technical aspect system deficiency customer familiar operation participated video teleconference conference call briefing designed address customer dfas management requirement often called upon explain layman term dfas policy procedure regard delay payment due various reason required respond various audit report study ensuring senior management audit personnel understand particular situation within vendor pay business process result finding hand selected field site director vendor pay site manager represent dfas limestone team comprised representative dfas denver field site provide training air force base level resource advisor five week period provided boot camp training base level personnel ensuring resource advisor familiar dfas structure mission requirement related fund management company name june june accountant network assistant team city state member network assistance team required extensive working knowledge dod accounting system theory policy procedure consistently called upon develop implement procedure consistent dod regulation coordinated dfas denver omaha field site consolidation first geographically separated defense accounting office dfas position member network assistance team required upon arrival base level defense accounting office dao provide brief briefing identified team member purpose visit goal responsibility upon completion assignment provided written oral brief outlining team accomplishment visit provided recommendation preclude recurring problem prepare organization consolidation company name june june supervisor account control branch city state directed supervised accomplishment financial report statement directed supervised accomplishment financial report statement responsible completeness accuracy weekly monthly quarterly semi annual annual report monitored error general accounting finance system gafs bq ensured corrective action accomplished also ensured fund balance reconciled appropriate audit listing verified report prior release base activity higher headquarters furnished accounting data base organization often interpreting analyzing data help fund manager resolve problem manage program effectively attended major command majcom headquarters level workshop participate contribute accounting policy system change provided professional assistance data automation relevant processing accounting finance data interpreting deficiency software based output product system related problem utilized working knowledge commercial government accounting system principle knowledge processing center pc review verify analyze evaluate accounting finance operation serving chief account control ensured area concern addressed concentrating problem area related database analyzed computer output product determine processing deficiency included limited open document listing odl operating budget ledger obl allotment ledger al accounting finance workload information management system f wims extract list provided technical assistance related policy procedural change required result impending base closure analyzed developed recommended improved training procedure enabling better use system procedure ensuring governing directive followed evaluated accuracy accounting record prior fiscal year closeout ensuring accounting finance officer could certify accuracy required regulation examined accounting transaction document ensure conformed established accounting policy principle coordinated directed fiscal year end conversion gafs integrated account payable system iaps education northern maine community college associate accounting city state usa emphasis business associate accounting city state usa gpa gpa accounting gpa hour quarter attended husson college major accounting semester hour toward bachelor degree professional military comptroller school wk managerial accounting interested based bargaining training management hr auditing method concept organizational leadership hr management development ii hr certification certified defense financial manager cdfm may interest american society military comptroller additional information skill accounting general accounting account payable program management\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean, preprocess, and lemmatize text.\"\"\"\n",
    "    if pd.isna(text):  # Handle missing values\n",
    "        return \"\"\n",
    "\n",
    "    # Remove special characters, numbers, and extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra spaces\n",
    "    text = re.sub(r\"\\W+\", \" \", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\d+\", \" \", text)  # Remove numbers\n",
    "    \n",
    "    # Convert to lowercase and strip spaces\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "# Apply text cleaning\n",
    "data_df[\"Cleaned_Text\"] = data_df[\"Resume_Text\"].apply(clean_text)\n",
    "\n",
    "print(data_df['Cleaned_Text'][0])  # Verify cleaned text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e93c6e",
   "metadata": {},
   "source": [
    "### 4. Building LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be85ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e230408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.2, random_state=42, stratify=data_df['Category'])\n",
    "\n",
    "X_train = train_df['Cleaned_Text'].values\n",
    "y_train = train_df['Category'].values\n",
    "X_test = test_df['Cleaned_Text'].values\n",
    "y_test = test_df['Category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5038cd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1987,), (1987,), (497,), (497,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a6e7b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Label Mapping: {'ACCOUNTANT': np.int64(0), 'ADVOCATE': np.int64(1), 'AGRICULTURE': np.int64(2), 'APPAREL': np.int64(3), 'ARTS': np.int64(4), 'AUTOMOBILE': np.int64(5), 'AVIATION': np.int64(6), 'BANKING': np.int64(7), 'BPO': np.int64(8), 'BUSINESS-DEVELOPMENT': np.int64(9), 'CHEF': np.int64(10), 'CONSTRUCTION': np.int64(11), 'CONSULTANT': np.int64(12), 'DESIGNER': np.int64(13), 'DIGITAL-MEDIA': np.int64(14), 'ENGINEERING': np.int64(15), 'FINANCE': np.int64(16), 'FITNESS': np.int64(17), 'HEALTHCARE': np.int64(18), 'HR': np.int64(19), 'INFORMATION-TECHNOLOGY': np.int64(20), 'PUBLIC-RELATIONS': np.int64(21), 'SALES': np.int64(22), 'TEACHER': np.int64(23)}\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Get label mapping\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Category Label Mapping:\", label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7e2e07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Data: (1987, 3309)\n",
      "Shape of Testing Data: (497, 3309)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=5000)  # Limit vocabulary size\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text into sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure equal length\n",
    "max_len = max(len(seq) for seq in X_train_seq)  # Find max sequence length\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding=\"post\")\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "print(\"Shape of Training Data:\", X_train_pad.shape)\n",
    "print(\"Shape of Testing Data:\", X_test_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fc4b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResumeDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(texts, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = ResumeDataset(X_train_pad, y_train_encoded)\n",
    "test_dataset = ResumeDataset(X_test_pad, y_test_encoded)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72daaa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUClassifier(\n",
      "  (embedding): Embedding(5000, 128)\n",
      "  (gru1): GRU(128, 64, batch_first=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (gru2): GRU(64, 32, batch_first=True)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=32, out_features=24, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=5000, embedding_dim=128, hidden_dim=64, output_dim=24, dropout_prob=0.2):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru1 = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.gru2 = nn.GRU(hidden_dim, hidden_dim // 2, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim // 2, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x.long())\n",
    "        x = self.dropout1(x)\n",
    "        gru_out1, _ = self.gru1(x)\n",
    "        gru_out1 = self.dropout1(gru_out1)\n",
    "        gru_out2, _ = self.gru2(gru_out1)\n",
    "        out = self.dropout2(gru_out2[:, -1, :])\n",
    "        return self.fc(out)\n",
    "\n",
    "# Instantiate model\n",
    "model = GRUClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5b4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier(\n",
      "  (embedding): Embedding(5000, 128)\n",
      "  (lstm1): LSTM(128, 64, batch_first=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (lstm2): LSTM(64, 32, batch_first=True)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=32, out_features=24, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# class LSTMClassifier(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout_prob=0.2):\n",
    "#         super(LSTMClassifier, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "#         self.dropout1 = nn.Dropout(dropout_prob)\n",
    "#         self.lstm2 = nn.LSTM(hidden_dim, hidden_dim // 2, batch_first=True)\n",
    "#         self.dropout2 = nn.Dropout(dropout_prob)\n",
    "#         self.fc = nn.Linear(hidden_dim // 2, output_dim)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x.long())\n",
    "#         x = self.dropout1(x)\n",
    "#         lstm_out1, _ = self.lstm1(x)\n",
    "#         lstm_out1 = self.dropout1(lstm_out1)\n",
    "#         lstm_out2, _ = self.lstm2(lstm_out1)\n",
    "#         out = self.dropout2(lstm_out2[:, -1, :])\n",
    "#         return self.fc(out)\n",
    "\n",
    "# # Model parameters\n",
    "# vocab_size = 5000\n",
    "# embedding_dim = 128\n",
    "# hidden_dim = 64\n",
    "# output_dim = len(label_mapping)\n",
    "\n",
    "# # Instantiate model\n",
    "# model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44d2fb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 200.4157\n",
      "Epoch 2/10, Loss: 200.3176\n",
      "Epoch 3/10, Loss: 199.2496\n",
      "Epoch 4/10, Loss: 199.3876\n",
      "Epoch 5/10, Loss: 199.5937\n",
      "Epoch 6/10, Loss: 199.2390\n",
      "Epoch 7/10, Loss: 199.0421\n",
      "Epoch 8/10, Loss: 198.5726\n",
      "Epoch 9/10, Loss: 198.8642\n",
      "Epoch 10/10, Loss: 198.7186\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for texts, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "684a5e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.05\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        outputs = model(texts)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e47ebf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: CHEF\n"
     ]
    }
   ],
   "source": [
    "sample_text = [\"Experienced financial analyst with expertise in investment banking.\"]\n",
    "sample_text = [clean_text(text) for text in sample_text]\n",
    "sample_seq = tokenizer.texts_to_sequences(sample_text)\n",
    "sample_pad = pad_sequences(sample_seq, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "sample_tensor = torch.tensor(sample_pad, dtype=torch.float32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(sample_tensor)\n",
    "    predicted_label = torch.argmax(output, dim=1).item()\n",
    "\n",
    "print(f\"Predicted Category: {label_encoder.inverse_transform([predicted_label])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73270e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUClassifier(\n",
       "  (embedding): Embedding(5000, 128)\n",
       "  (gru1): GRU(128, 64, batch_first=True)\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (gru2): GRU(64, 32, batch_first=True)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=32, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For clearing parameters\n",
    "def clear_parameters(model):\n",
    "    \"\"\"Clear model parameters to free up memory.\"\"\"\n",
    "    model.train(False)  # Set to evaluation mode\n",
    "    for param in model.parameters():\n",
    "        param.data.zero_()  # Zero out parameter data\n",
    "        param.grad = None   # Clear gradients\n",
    "        param.requires_grad = False  # Disable gradient computation\n",
    "    return model\n",
    "\n",
    "clear_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
