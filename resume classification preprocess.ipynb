{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import torch \n",
    "from pathlib import Path\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4adef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pdf = 'Resume Dataset/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9975d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdfs_to_dataframe(root_folder_path):\n",
    "    \"\"\"\n",
    "    Read all PDF files from folders and create a dataframe with occupation and resume columns.\n",
    "    \n",
    "    Args:\n",
    "        root_folder_path (str): Path to the root folder containing subfolders with PDF files\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with columns ['occupation', 'resume', 'file_path']\n",
    "    \"\"\"\n",
    "    pdf_data = []\n",
    "    root_path = Path(root_folder_path)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = root_path.rglob(\"*.pdf\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        strip_category = str(pdf_file).split('\\\\')[-2]  # Get the parent folder name as occupation\n",
    "        # print(strip_category)\n",
    "        try:\n",
    "            with open(pdf_file, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                \n",
    "                # Extract text from all pages\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                \n",
    "                # Split text into lines and filter out empty lines\n",
    "                lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "                \n",
    "                \n",
    "                if lines:\n",
    "                    occupation = lines[0]  # First row becomes occupation\n",
    "                    resume = '\\n'.join(lines[1:])  # Rest becomes resume\n",
    "                else:\n",
    "                    occupation = \"\"\n",
    "                    resume = \"\"\n",
    "                \n",
    "                pdf_data.append({\n",
    "                    'occupation': strip_category,\n",
    "                    'resume': resume\n",
    "                })\n",
    "                \n",
    "                print(f\"Successfully processed: {pdf_file}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {pdf_file}: {str(e)}\")\n",
    "            pdf_data.append({\n",
    "                'occupation': \"\",\n",
    "                'resume': \"\"\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    pdf_df = pd.DataFrame(pdf_data)\n",
    "    return pdf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_pdfs_to_dataframe(path_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c6ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)               # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z ]', ' ', text)            # Remove special chars\n",
    "    text = text.lower()                                # Lowercase\n",
    "    tokens = text.split()                              # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['resume'].apply(clean_and_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a79d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Resume Dataset/pdf_to_resume.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
